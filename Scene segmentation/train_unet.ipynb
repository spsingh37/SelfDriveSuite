{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import namedtuple\n",
    "from typing import Any, Callable, Dict, List, Optional, Union, Tuple\n",
    "# !pip install torch==2.0.0 torchvision==0.15.1 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu118\n",
    "print(torch.__version__)\n",
    "from unet import UNet\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root = r'C:\\Users\\Lenovo\\Downloads\\SelfDriveSuite\\Scene segmentation\\dataset_large'\n",
    "\n",
    "print(os.listdir(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Urbanscapes():\n",
    "    # Based on https://github.com/mcordts/cityscapesScripts\n",
    "    CityscapesClass = namedtuple(\n",
    "        \"CityscapesClass\",\n",
    "        [\"name\", \"id\", \"color\"],\n",
    "    )\n",
    "    \n",
    "    classes = [\n",
    "        CityscapesClass(\"unlabeled\", 0, (0, 0, 0)),\n",
    "        CityscapesClass(\"terrain\", 1, (210, 0, 200)),\n",
    "        CityscapesClass(\"sky\", 2, (90, 200, 255)),\n",
    "        CityscapesClass(\"tree\", 3, (0, 199, 0)),\n",
    "        CityscapesClass(\"vegetation\", 4, (90, 240, 0)),\n",
    "        CityscapesClass(\"building\", 5, (140, 140, 140)),\n",
    "        CityscapesClass(\"road\", 6, (100, 60, 100)),\n",
    "        CityscapesClass(\"guard rail\", 7, (250, 100, 255)),\n",
    "        CityscapesClass(\"traffic sign\", 8, (255, 255, 0)),\n",
    "        CityscapesClass(\"traffic light\", 9, (200, 200, 0)),\n",
    "        CityscapesClass(\"pole\", 10, (255, 130, 0)),\n",
    "        CityscapesClass(\"misc\", 11, (80, 80, 80)),\n",
    "        CityscapesClass(\"truck\", 12, (160, 60, 60)),\n",
    "        CityscapesClass(\"car\", 13, (255, 127, 80)),\n",
    "        CityscapesClass(\"van\", 14, (0, 139, 139)),\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        mode=\"train\",\n",
    "        target_type= \"instance\",\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "        transforms: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "        self.root = root\n",
    "        self.mode = mode \n",
    "        self.transforms = transforms\n",
    "        self.target_transform = target_transform\n",
    "        self.images_dir = os.path.join(self.root, self.mode, \"images\")\n",
    "        self.targets_dir = os.path.join(self.root, self.mode, \"targets\")\n",
    "        self.target_type = target_type\n",
    "        self.images = []\n",
    "        self.targets = []\n",
    "\n",
    "        # print(os.listdir(self.images_dir))\n",
    "        for city in os.listdir(self.images_dir):\n",
    "            img_dir = self.images_dir\n",
    "            target_dir = self.targets_dir\n",
    "            for file_name in os.listdir(img_dir):\n",
    "                self.images.append(os.path.join(img_dir, file_name))\n",
    "                self.targets.append(os.path.join(target_dir, file_name))\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is a tuple of all target types if target_type is a list with more\n",
    "            than one item. Otherwise, target is a json object if target_type=\"polygon\", else the image segmentation.\n",
    "        \"\"\"\n",
    "\n",
    "        image = Image.open(self.images[index]).convert(\"RGB\")\n",
    "\n",
    "        target = Image.open(self.targets[index])\n",
    "        if self.transforms is not None:\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Urbanscapes(root=root, mode='train', target_type='semantic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(ncols=2,figsize=(12,8))\n",
    "ax[0].imshow(dataset[0][0])\n",
    "ax[1].imshow(dataset[0][1],cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some utility functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_index=0\n",
    "valid_classes = [ignore_index, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "class_names = ['unlabelled', 'terrain', 'sky', 'tree', 'vegetation', 'building', 'road', 'guard_rail', \\\n",
    "               'traffic_sign', 'traffic_light', 'pole', 'misc', 'truck', 'car', 'van']\n",
    "\n",
    "#why i choose 15 classes\n",
    "#https://stackoverflow.com/a/64242989\n",
    "\n",
    "class_map = dict(zip(valid_classes, range(len(valid_classes))))\n",
    "n_classes=len(valid_classes)\n",
    "class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [ [0, 0, 0],\n",
    "        [210, 0, 200],     \n",
    "        [90, 200, 255],    \n",
    "        [0, 199, 0],       \n",
    "        [90, 240, 0],      \n",
    "        [140, 140, 140],   \n",
    "        [100, 60, 100],    \n",
    "        [250, 100, 255],   \n",
    "        [255, 255, 0],     \n",
    "        [200, 200, 0],     \n",
    "        [255, 130, 0],    \n",
    "        [80, 80, 80],     \n",
    "        [160, 60, 60],    \n",
    "        [255, 127, 80],   \n",
    "        [0, 139, 139],     \n",
    "    ]\n",
    "\n",
    "label_colours = dict(zip(range(n_classes), colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_segmap(mask):\n",
    "    #remove unwanted classes and recitify the labels of wanted classes\n",
    "    # for _voidc in void_classes:\n",
    "    #     mask[mask == _voidc] = ignore_index\n",
    "    for _validc in valid_classes:\n",
    "        mask[mask == _validc] = class_map[_validc]\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_segmap(temp):\n",
    "    #convert gray scale to color\n",
    "    temp=temp.numpy()\n",
    "    r = temp.copy()\n",
    "    g = temp.copy()\n",
    "    b = temp.copy()\n",
    "    for l in range(0, n_classes):\n",
    "        r[temp == l] = label_colours[l][0]\n",
    "        g[temp == l] = label_colours[l][1]\n",
    "        b[temp == l] = label_colours[l][2]\n",
    "\n",
    "    rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n",
    "    rgb[:, :, 0] = r / 255.0\n",
    "    rgb[:, :, 1] = g / 255.0\n",
    "    rgb[:, :, 2] = b / 255.0\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "transform=A.Compose(\n",
    "[\n",
    "    A.Resize(256, 512),\n",
    "    A.HorizontalFlip(),\n",
    "    A.RandomSizedCrop(min_max_height=(64, 128), size=(256, 512), p=0.5),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "]\n",
    ")\n",
    "val_transform=A.Compose(\n",
    "[\n",
    "    A.Resize(256, 512),\n",
    "    ToTensorV2(),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional, Union, Tuple\n",
    "\n",
    "\n",
    "class Loaddataclass(Urbanscapes):\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        image = Image.open(self.images[index]).convert('RGB')\n",
    "\n",
    "        target = Image.open(self.targets[index])\n",
    "        if self.transforms is not None:\n",
    "            transformed=transform(image=np.array(image), mask=np.array(target))            \n",
    "        return transformed['image'],transformed['mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Loaddataclass(root=root, mode='val', target_type='semantic',transforms=transform)\n",
    "img,seg= dataset[0]\n",
    "print(img.shape,seg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(ncols=2,nrows=1,figsize=(16,8))\n",
    "ax[0].imshow(img.permute(1, 2, 0))\n",
    "ax[1].imshow(seg,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class labels before label correction\n",
    "print(torch.unique(seg))\n",
    "print(len(torch.unique(seg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class labels after label correction\n",
    "res=encode_segmap(seg.clone())\n",
    "print(res.shape)\n",
    "print(torch.unique(res))\n",
    "print(len(torch.unique(res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let do coloring\n",
    "res1=decode_segmap(res.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(ncols=2,figsize=(12,10))  \n",
    "ax[0].imshow(res,cmap='gray')\n",
    "ax[1].imshow(res1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Loaddataclass(root=root, mode='train', target_type='semantic',transforms=transform)\n",
    "\n",
    "dataset_valid = Loaddataclass(root=root, mode='val', target_type='semantic',transforms=val_transform)\n",
    "\n",
    "# dataloader training\n",
    "dataloader_training = DataLoader(dataset=dataset_train, batch_size=2, shuffle=True)\n",
    "\n",
    "# dataloader validation\n",
    "dataloader_validation = DataLoader(dataset=dataset_valid, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "class MulticlassDiceLoss(nn.Module):\n",
    "    \"\"\"Reference: https://www.kaggle.com/code/bigironsphere/loss-function-library-keras-pytorch#Dice-Loss\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, softmax_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.softmax_dim = softmax_dim\n",
    "    def forward(self, logits, targets, reduction='mean', smooth=1e-6):\n",
    "        \"\"\"The \"reduction\" argument is ignored. This method computes the dice\n",
    "        loss for all classes and provides an overall weighted loss.\n",
    "        \"\"\"\n",
    "        probabilities = logits\n",
    "        if self.softmax_dim is not None:\n",
    "            probabilities = nn.Softmax(dim=self.softmax_dim)(logits)\n",
    "        # end if\n",
    "        targets_one_hot = torch.nn.functional.one_hot(targets, num_classes=self.num_classes)\n",
    "        \n",
    "        # Convert from NHWC to NCHW\n",
    "        targets_one_hot = targets_one_hot.permute(0, 3, 1, 2)\n",
    "        # print(targets_one_hot.shape)\n",
    "        # Multiply one-hot encoded ground truth labels with the probabilities to get the\n",
    "        # prredicted probability for the actual class.\n",
    "        intersection = (targets_one_hot * probabilities).sum()\n",
    "        \n",
    "        mod_a = intersection.sum()\n",
    "        mod_b = targets.numel()\n",
    "        \n",
    "        dice_coefficient = 2. * intersection / (mod_a + mod_b + smooth)\n",
    "        dice_loss = -dice_coefficient.log()\n",
    "        # dice_loss = 1 - dice_coefficient\n",
    "        return dice_loss, dice_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "import torchmetrics\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        device: torch.device,\n",
    "        criterion: torch.nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        training_dataloader: Dataset,\n",
    "        validation_dataloader: None,\n",
    "        lr_scheduler: None,\n",
    "        epochs: int = 100,\n",
    "        epoch: int = 0,\n",
    "        notebook: bool = False,\n",
    "        checkpoint_dir: str = 'experiment',\n",
    "        save_frequency: int = 1\n",
    "    ):\n",
    "\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.training_dataloader = training_dataloader\n",
    "        self.validation_dataloader = validation_dataloader\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.epoch = epoch\n",
    "        self.notebook = notebook\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.save_frequency = save_frequency\n",
    "        self.training_loss = []\n",
    "        self.validation_loss = []\n",
    "        self.learning_rate = []\n",
    "        self.data = []\n",
    "        self.checkpoint_dir = f\"checkpoints/{self.checkpoint_dir}_{time.time()}\"\n",
    "        self.best_iou = 0.00001\n",
    "        self.metrics = torchmetrics.classification.MulticlassJaccardIndex(num_classes=15).to(device)\n",
    "        if not os.path.exists(self.checkpoint_dir):\n",
    "            os.makedirs(self.checkpoint_dir)\n",
    "\n",
    "    def run_trainer(self):\n",
    "\n",
    "        if self.notebook:\n",
    "            from tqdm.notebook import tqdm, trange\n",
    "        else:\n",
    "            from tqdm import tqdm, trange\n",
    "\n",
    "        progressbar = trange(self.epochs, desc=\"Progress\")\n",
    "        for i in progressbar:\n",
    "            \"\"\"Epoch counter\"\"\"\n",
    "            self.epoch += 1  # epoch counter\n",
    "            print(\"epoch: \", self.epoch)\n",
    "            \"\"\"Training block\"\"\"\n",
    "            self._train()\n",
    "\n",
    "            # if self.epoch % self.save_frequency == 0:\n",
    "            #     model_name = 'epoch_' + str(self.epoch)+'.pth'\n",
    "            #     torch.save(self.model.state_dict(), os.path.join(self.checkpoint_dir, model_name))\n",
    "            \n",
    "\n",
    "            \"\"\"Validation block\"\"\"\n",
    "            if self.validation_dataloader is not None:\n",
    "                iou_value = self._validate()\n",
    "\n",
    "            if iou_value > self.best_iou:\n",
    "                model_name = 'epoch_best.pth'\n",
    "                torch.save(self.model.state_dict(), os.path.join(self.checkpoint_dir, model_name))\n",
    "                print(f\"weights saved at epoch: {self.epoch}\")\n",
    "                self.best_iou = iou_value\n",
    "\n",
    "            \"\"\"Learning rate scheduler block\"\"\"\n",
    "            if self.lr_scheduler is not None:\n",
    "                if (\n",
    "                    self.validation_dataloader is not None\n",
    "                    and self.lr_scheduler.__class__.__name__ == \"ReduceLROnPlateau\"\n",
    "                ):\n",
    "                    self.lr_scheduler.step(\n",
    "                        self.validation_loss[i]\n",
    "                    )  # learning rate scheduler step with validation loss\n",
    "                else:\n",
    "                    self.lr_scheduler.step()\n",
    "\n",
    "                    #self.lr_scheduler.batch()  # learning rate scheduler step\n",
    "        return self.training_loss, self.validation_loss, self.learning_rate, self.data\n",
    "\n",
    "    def _train(self):\n",
    "\n",
    "        if self.notebook:\n",
    "            from tqdm.notebook import tqdm, trange\n",
    "        else:\n",
    "            from tqdm import tqdm, trange\n",
    "\n",
    "        self.model.train()  # train mode\n",
    "        train_losses = []  # accumulate the losses here\n",
    "        train_iou = []\n",
    "        torch.manual_seed(0)\n",
    "        batch_iter = tqdm(\n",
    "            enumerate(self.training_dataloader),\n",
    "            \"Training\",\n",
    "            total=len(self.training_dataloader),\n",
    "            leave=False,\n",
    "            disable=True,\n",
    "        )\n",
    "        torch.manual_seed(0)\n",
    "        for i, (x, y) in batch_iter:\n",
    "            input_x, target_y = x.to(self.device), encode_segmap(y.to(\n",
    "                self.device)\n",
    "            )  # send to device (GPU or CPU)\n",
    "            self.optimizer.zero_grad()  # zerograd the parameters\n",
    "            # print(\"input_x shape: \", input_x.shape)\n",
    "            out = self.model(input_x)  # one forward pass\n",
    "            # print(\"train out.shape: \", out.shape)\n",
    "            out = np.squeeze(out, axis=1)\n",
    "            # print(\"loss computation\")\n",
    "            # print(\"input_x.shape: \", input_x.shape)\n",
    "            # print(\"target_y.shape: \", target_y.shape)\n",
    "            self.data = [input_x, target_y, out]\n",
    "            # print(\"train out.shape: \", out.shape)\n",
    "            loss, dice_score = self.criterion(out, target_y.long())  # calculate loss\n",
    "            loss_value = loss.item()\n",
    "            train_losses.append(loss_value)\n",
    "            # print(\"train loss_value: \", loss_value)\n",
    "            iou = self.metrics(out, target_y)\n",
    "            iou_value = iou.item()\n",
    "            train_iou.append(iou_value)\n",
    "            # print(\"train_iou: \", iou_value)\n",
    "            # if i%200==0:\n",
    "            #     print(\"train loss_value: \", np.mean(train_losses))\n",
    "            #     print(\"train iou_value: \", np.mean(train_iou))\n",
    "            #     if i%4000==0:\n",
    "            #         print(\"validating now..\")\n",
    "            #         iou_value = self._validate()\n",
    "            #         if iou_value > self.best_iou:\n",
    "            #             model_name = 'epoch_best.pth'\n",
    "            #             torch.save(self.model.state_dict(), os.path.join(self.checkpoint_dir, model_name))\n",
    "            #             print(f\"weights saved at epoch: {self.epoch}\")\n",
    "            #             self.best_iou = iou_value\n",
    "            # print(\"train iou: \", iou)\n",
    "            # print(\"train dice_score: \", dice_score)\n",
    "            loss.backward()  # one backward pass\n",
    "            self.optimizer.step()  # update the parameters\n",
    "\n",
    "            batch_iter.set_description(\n",
    "                f\"Training: (loss {loss_value:.4f})\"\n",
    "            )  # update progressbar\n",
    "            \n",
    "\n",
    "        self.training_loss.append(np.mean(train_losses))\n",
    "        print(\"train mean loss_value: \", np.mean(train_losses))\n",
    "        print(\"train mean iou_value: \", np.mean(train_iou))\n",
    "        self.learning_rate.append(self.optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "        batch_iter.close()\n",
    "\n",
    "    def _validate(self):\n",
    "\n",
    "        if self.notebook:\n",
    "            from tqdm.notebook import tqdm, trange\n",
    "        else:\n",
    "            from tqdm import tqdm, trange\n",
    "\n",
    "        self.model.eval()  # evaluation mode\n",
    "        valid_losses = []  # accumulate the losses here\n",
    "        valid_iou = []\n",
    "        batch_iter = tqdm(\n",
    "            enumerate(self.validation_dataloader),\n",
    "            \"Validation\",\n",
    "            total=len(self.validation_dataloader),\n",
    "            leave=False,\n",
    "            disable=True,\n",
    "        )\n",
    "\n",
    "        for i, (x, y) in batch_iter:\n",
    "            input, target = x.to(self.device), encode_segmap(y.to(\n",
    "                self.device)\n",
    "            )  # send to device (GPU or CPU)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = self.model(input)\n",
    "                # print(out.shape)\n",
    "                loss, dice_score = self.criterion(out, target.long())\n",
    "                loss_value = loss.item()\n",
    "                valid_losses.append(loss_value)\n",
    "                iou = self.metrics(out, target)\n",
    "                iou_value = iou.item()\n",
    "                valid_iou.append(iou_value)\n",
    "                batch_iter.set_description(f\"Validation: (loss {loss_value:.4f})\")\n",
    "\n",
    "\n",
    "        self.validation_loss.append(np.mean(valid_losses))\n",
    "        print(\"val loss_value: \", np.mean(valid_losses))\n",
    "        mean_iou = np.mean(valid_iou)\n",
    "        print(\"val iou_value: \", mean_iou)\n",
    "        batch_iter.close()\n",
    "        return mean_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# model\n",
    "model = UNet(\n",
    "    in_channels=3,\n",
    "    out_channels=15,\n",
    "    n_blocks=4,\n",
    "    start_filters=128,\n",
    "    activation=\"relu\",\n",
    "    normalization=\"batch\",\n",
    "    conv_mode=\"same\",\n",
    "    dim=2,\n",
    ").to(device)\n",
    "\n",
    "criterion = MulticlassDiceLoss(num_classes=15, softmax_dim=1)\n",
    "# optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.004) #lr=0.0025 min_lr=0\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.4, patience=15, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0.0001, eps=1e-08, verbose=False)\n",
    "\n",
    "# trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    training_dataloader=dataloader_training,\n",
    "    validation_dataloader=dataloader_validation,\n",
    "    lr_scheduler=scheduler,\n",
    "    epochs=100,\n",
    "    epoch=0,\n",
    "    notebook=True,\n",
    "    checkpoint_dir=f\"experiment_lr_on_plateau_dataset_large_color_aug\",\n",
    "    save_frequency=1\n",
    ")\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(count_parameters(model=model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_losses, validation_losses, lr_rates, data = trainer.run_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual import plot_training\n",
    "\n",
    "fig = plot_training(\n",
    "    training_losses,\n",
    "    validation_losses,\n",
    "    lr_rates,\n",
    "    gaussian=True,\n",
    "    sigma=1,\n",
    "    figsize=(10, 4),\n",
    ")\n",
    "print(validation_losses)\n",
    "print(training_losses)\n",
    "print(f\"best weights at epoch {np.argmin(validation_losses)+1} having loss {np.min(validation_losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing (& Saving the predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(target, prediction, iteration, output_dir=\"output_images\"):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # probabilities\n",
    "    \n",
    "    # Decode the target and prediction\n",
    "    decoded_target = decode_segmap(target.cpu())\n",
    "    # Get the predicted classes by taking argmax along the channel dimension\n",
    "    predicted_classes = torch.argmax(prediction, dim=0)\n",
    "    \n",
    "    # Decode the prediction\n",
    "    decoded_prediction = decode_segmap(predicted_classes.cpu())\n",
    "    # decoded_prediction = decode_segmap(prediction.cpu())\n",
    "    \n",
    "    # Save target image\n",
    "    plt.imsave(os.path.join(output_dir, f\"target_iter{iteration}.png\"), decoded_target)\n",
    "    \n",
    "    # Save prediction image\n",
    "    plt.imsave(os.path.join(output_dir, f\"prediction_iter{iteration}.png\"), decoded_prediction)\n",
    "\n",
    "root = r\"C:\\Users\\Lenovo\\Downloads\\SelfDriveSuite\\Scene segmentation\\dataset_large\\test_data\"\n",
    "# dataset_valid = Loaddataclass(root=root, mode='val', target_type='semantic',transforms=transform)\n",
    "# dataset = Urbanscapes(root=root, mode='val', target_type='semantic')\n",
    "transform=A.Compose(\n",
    "[\n",
    "    A.Resize(256, 512),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "]\n",
    ")\n",
    "dataset_valid = Loaddataclass(root=root, mode='val', target_type='semantic',transforms=transform)\n",
    "\n",
    "# dataloader validation\n",
    "dataloader_validation = DataLoader(dataset=dataset_valid, batch_size=1, shuffle=False)\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "# model.eval()  # evaluation mode\n",
    "model.load_state_dict(torch.load(r'C:\\Users\\Lenovo\\Downloads\\SelfDriveSuite\\Scene segmentation\\checkpoints\\experiment_lr_on_plateau_dataset_large_color_aug\\epoch_best.pth'))\n",
    "model.eval()  # evaluation mode\n",
    "valid_losses = []  # accumulate the losses here\n",
    "valid_iou = []\n",
    "batch_iter = tqdm(\n",
    "    enumerate(dataloader_validation),\n",
    "    \"Validation\",\n",
    "    total=len(dataloader_validation),\n",
    "    leave=False,\n",
    "    disable=True,\n",
    ")\n",
    "print(len(dataloader_validation))\n",
    "metrics = torchmetrics.classification.MulticlassJaccardIndex(num_classes=15).to(device)\n",
    "for i, (x, y) in batch_iter:\n",
    "    input, target = x.to(device), encode_segmap(y.to(\n",
    "        device)\n",
    "    )  # send to device (GPU or CPU)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(input)\n",
    "        print(out.shape)\n",
    "        print(target.shape)\n",
    "        # loss, dice_score = criterion(out, target.long())\n",
    "        # loss_value = loss.item()\n",
    "        # valid_losses.append(loss_value)\n",
    "        iou = metrics(out, target)\n",
    "        iou_value = iou.item()\n",
    "        print(f\"i: {i}; iou_value: \", iou_value)\n",
    "        valid_iou.append(iou_value)\n",
    "        # batch_iter.set_description(f\"Validation: (loss {loss_value:.4f})\")\n",
    "        target = np.squeeze(target, axis = 0)\n",
    "        out = np.squeeze(out, axis = 0)\n",
    "        save_image(target, out, i)\n",
    "        if i>=len(batch_iter)/(339*1)-1:\n",
    "            break\n",
    "\n",
    "\n",
    "# print(\"val loss_value: \", np.mean(valid_losses))\n",
    "mean_iou = np.mean(valid_iou)\n",
    "print(\"val iou_value: \", mean_iou)\n",
    "batch_iter.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
